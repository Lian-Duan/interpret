{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EBMs Overall Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to compute and interpret Overall Importances shown in EBMs Global Explanations. We also show how to compute importances of a group of terms, i.e. composite importances.\n",
    "\n",
    "Throughout the notebook we use _term_ to denote both single features and interactions (pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an Explainable Boosting Machine (EBM) for a regression task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the california housing price dataset as a reference and train an EBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "\n",
    "cal = fetch_california_housing()\n",
    "\n",
    "ebm = ExplainableBoostingRegressor(feature_names=cal.feature_names)\n",
    "ebm.fit(cal.data, cal.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EBMs provide explanations on a both global (overall behavior) and local (individual predictions) levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Explanations are useful for understanding what a model finds important, as well as identifying potential flaws in its decision making. Let's start by computing and displayng a global explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import show\n",
    "\n",
    "ebm_global = ebm.explain_global(name='EBM')\n",
    "show(ebm_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because EBMs are additive models, we can measure exactly how much each term contributes to a prediction. Let's take a look at the graph of the first term, `MedInc`, by selecting it in the above drop-down menu.\n",
    "\n",
    "The way to interpret this is that if a new datapoint came in with `MedInc` = 10, the model adds about +2 to their final prediction. However, for a different datapoint with `MedInc` = 4, the model would now adds ~0 to the prediction, and for datapoints that have `MedInc` = 2, the model adds approx. -0.6.\n",
    "\n",
    "To make individual predictions, the model uses each term graph as a look up table, notes the contribution per term, and sums them together with the learned intercept to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the full breakdown for any individual prediction with Local Explanations. Here's the prediction breakdown for the first sample in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import show\n",
    "show(ebm.explain_local(cal.data[:1], cal.target[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model prediction is 4.04. We can see that the intercept adds about +2, MedInc adds ~+1.8, and the Longitude adds about +0.4. So far, for the top 3 contributing terms, we're at a cumulative prediction of ~+4.2. If you repeat this process for all the features, you'll arrive exactly at the model prediction of +4.04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the global explanation's Summary, the overall importance is calculated as _the average absolute contribution (score) a term (feature or pair) makes when predicting across the training dataset._\n",
    "\n",
    "In our example above, we saw that `MedInc` contributed a +1.8 for this datapoint. If we repeat this process for all datapoints in the training set we'll arrive at the  `MedInc`'s overall importance. This can be interpreted as _on average, across the training dataset, how much does `MedInc` contribute to a single prediction in absolute terms?_\n",
    "\n",
    "In other words, the overall importance is not a measure of positive/negative -- it is a measure of how important each term is in the score overall. These scores are represented in the same units as the y-axis of the feature graphs, so for a classification problem it would be in logits and for regression the original label space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing overall importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the overall importances of a trained EBM we use `get_importances()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = ebm.get_importances()\n",
    "names = ebm.get_feature_names_out()\n",
    "\n",
    "for (term_name, importance) in zip(names, importances):\n",
    "    print(f\"Term {term_name} importance: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this isn't the only way of calculating feature importance. Another metric our package provide is the 'max\" option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = ebm.get_importances(\"max\")\n",
    "names = ebm.get_feature_names_out()\n",
    "\n",
    "for (term, importance) in zip(names, importances):\n",
    "    print(f\"Term {term} importance: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composite Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A composite is a set of terms. We provide utility functions to compute the importances of composites and, optionally, append them to global explanations. Note that no individual graphs are generated, just its overall importance is shown on Summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing composite importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list of terms -- single features or interactions -- as our composite and then compute its importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox.ebm.research.composite_importance import *\n",
    "\n",
    "composite_terms_1 = [\"MedInc\", \"Population\", \"Latitude x Longitude\"]\n",
    "importance = compute_composite_importance(composite_terms_1, ebm, cal.data)\n",
    "print(f\"Composite: {composite_terms_1} - Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we create a composite with three terms, 2 features (`MedInc` and `Population`) and 1 interaction (`Latitude x Longitude`), and compute its importance. Similar to single feature importances, we interpret this score as _the average absolute contribution this set of terms makes when predicting across the training dataset._ \n",
    "\n",
    "Note that for each prediction, the contribution of each term in the composite will be added before taking the absolute value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the option to create a global explanation containing the composite importance or append it to an existing explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_global_exp = append_composite_importance(composite_terms_1, ebm, cal.data)\n",
    "show(my_global_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of `composite_terms_1` is about 0.53, which is higher than any individual importance. We could make this type of comparison between different composites too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_terms_2 = [\"AveRooms\", \"HouseAge\"]\n",
    "my_global_exp = append_composite_importance(composite_terms_2, ebm, cal.data, global_exp=my_global_exp)\n",
    "show(my_global_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of `composite_terms_2` is about 0.11, higher than each of its terms but smaller then other important terms such as `Longitude`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare one composite we are interested in (e.g. `composite_terms_1`) and a composite of all other terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_other_terms = [term for term in ebm.get_feature_names_out() if term not in composite_terms_1]\n",
    "\n",
    "my_global_exp = append_composite_importance(composite_terms_1, ebm, cal.data)\n",
    "my_global_exp = append_composite_importance(all_other_terms, ebm, cal.data, composite_name=\"all_other_terms\", global_exp=my_global_exp)\n",
    "show(my_global_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `composite_terms_1` still has the highest importance score. Moreover, although `Latitude` is one of the terms in `all_other_terms`, its individual importance is higher than the composite itself -- this is possible because when we consider a composite, for each prediction, all the scores of the composite terms are added before taking the absolute value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also expose a function to compute the importances of a list of composite terms as well as all the model's original terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = get_composite_and_individual_terms([composite_terms_1, composite_terms_2], ebm, cal.data)\n",
    "for key in my_dict:\n",
    "    print(f\"Term: {key} - Importance: {my_dict[key]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7128c28a1fbf56f7b0ff727a5bfcd1f6d1f6ebc3f8bf579bb6fc95a9324d5117"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('interpret')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
